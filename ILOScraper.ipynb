{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import urllib2\n",
    "import urllib\n",
    "from bs4 import BeautifulSoup\n",
    "from pymarc import Record, Field, XMLWriter\n",
    "import json\n",
    "import os.path\n",
    "import sys\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NEEDS CHANGING TO KEEP UP WITH TOTAL PAGE COUNT AT http://www.ilo.org/global/publications/books/lang--en/index.htm\n",
    "pagesToIndex = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cleanListFile = './ilo/.state/ILOCleanUrlList.json' # List of URLs already processed\n",
    "errorListFile = './ilo/.state/ILOErrorUrlList.json' # List of URLs that cause problems\n",
    "\n",
    "downloadLocation = './ilo/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getParsedHTML(url):\n",
    "    url = urllib2.urlopen(url)\n",
    "    html = url.read()\n",
    "    url.close()\n",
    "    return BeautifulSoup(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not os.path.isfile(cleanListFile):\n",
    "    with open(cleanListFile, 'w') as fp:\n",
    "        json.dump([], fp)\n",
    "\n",
    "if not os.path.isfile(errorListFile):\n",
    "    with open(errorListFile, 'w') as fp:\n",
    "        json.dump([], fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "incomingUrlList = []\n",
    "\n",
    "# Scrape url list from ILO puplication list\n",
    "for i in xrange(0, pagesToIndex*10, 10):\n",
    "    # print str((float(i)/pagesToIndex)*10) + \"% Comlpete\" # Print percentage complete\n",
    "    parsed_html = getParsedHTML(\"http://www.ilo.org/global/publications/books/lang--en/nextRow--\"+str(i)+\"/index.htm\")\n",
    "    liList = parsed_html.body.find('div', attrs={'class':'items-list'}).find_all('li')\n",
    "    for li in liList:\n",
    "        incomingUrlList.append('http://www.ilo.org' + li.a['href'])\n",
    "\n",
    "# Check scraped list against urls for books that have already been scraped for metadata\n",
    "with open(cleanListFile) as fp:\n",
    "    cleanUrlList = json.load(fp)\n",
    "with open(errorListFile) as fp:\n",
    "    errorUrlList = json.load(fp)\n",
    "\n",
    "dirtyUrlList = []\n",
    "# If book already scraped, delete\n",
    "for i, url in enumerate(incomingUrlList):\n",
    "    if url not in cleanUrlList and url not in errorUrlList:\n",
    "        dirtyUrlList.append(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bookMetaData = []\n",
    "\n",
    "for i, url in enumerate(dirtyUrlList):\n",
    "    # parse HTML\n",
    "    parsed_html = getParsedHTML(url)\n",
    "    metaTemp = {}\n",
    "    try:\n",
    "        # If the returned URL actually has data\n",
    "        if parsed_html.find(class_=\"page-title\"):\n",
    "            metaTemp[u\"title\"] = parsed_html.find(class_=\"page-title\").h1.string\n",
    "            metaTemp[u'desc'] =  parsed_html.find(class_=\"page-title\").p.string\n",
    "        # Get publication metadata\n",
    "        pubData = parsed_html.find(class_=\"pub-data\").find_all('tr')\n",
    "        # Sort metadata\n",
    "        for tr in pubData:\n",
    "            metaTemp[tr.th.string[:-2].lower()] = tr.td.string\n",
    "        # Get PDF download link if it exists\n",
    "        if (parsed_html.find(id='download')):\n",
    "            metaTemp[u'downloadURL'] = 'http://www.ilo.org' + parsed_html.find(id='download').a['href']\n",
    "        # Record where all this data came from\n",
    "        metaTemp[u'originURL'] = url\n",
    "        metaTemp[u'publisher'] = 'ILO'\n",
    "    except:\n",
    "        # If there's an error, put it in error list\n",
    "        errorUrlList.append(url)\n",
    "    else:\n",
    "        # if there's no error, put the url on the clean list & save metadata\n",
    "        cleanUrlList.append(url)\n",
    "        bookMetaData.append(metaTemp)\n",
    "\n",
    "with open(errorListFile, 'wb') as fp:\n",
    "    json.dump(errorUrlList, fp)\n",
    "\n",
    "with open(cleanListFile, 'wb') as fp:\n",
    "    json.dump(cleanUrlList, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "downloadable = []\n",
    "for data in bookMetaData:\n",
    "    if \"downloadURL\" in data:\n",
    "        data[u'UUID'] = str(uuid.uuid1())\n",
    "        downloadable.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "MARCMapping = {u'UUID':'001', \n",
    "               u'title':'245$a', \n",
    "               u'authors':'100$a', \n",
    "               u'date issued':'260$c',\n",
    "               u'desc':'520$a',\n",
    "               u'publisher':'260$b',\n",
    "               u'reference':'020$a'}\n",
    "\n",
    "\n",
    "for data in downloadable:\n",
    "    url = data['downloadURL']\n",
    "    urllib.urlretrieve(url, downloadLocation + data['UUID'] + '.' + url.rsplit('.')[-1])\n",
    "    record = Record()\n",
    "    for key in data:\n",
    "        if key in MARCMapping:\n",
    "            if(len(MARCMapping[key].split('$')) == 1):\n",
    "                field = Field(\n",
    "                    tag = MARCMapping[key].split('$')[0],\n",
    "                    data = data[key])\n",
    "            else:\n",
    "                field = Field(\n",
    "                    tag = MARCMapping[key].split('$')[0],\n",
    "                    subfields = [MARCMapping[key].split('$')[1], data[key]],\n",
    "                    indicators=['0', '0'])  \n",
    "            record.add_field(field)\n",
    "    writer = XMLWriter(open(downloadLocation + data[u'UUID'] + '.xml', 'wb'))\n",
    "    writer.write(record)\n",
    "    writer.close()  # Important!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
