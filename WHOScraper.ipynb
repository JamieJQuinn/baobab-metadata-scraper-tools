{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import urllib2\n",
    "import urllib\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import os.path\n",
    "import sys\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dirtyListFile = 'WHODirtyUrlList.json'\n",
    "cleanListFile = 'WHOCleanUrlList.json'\n",
    "metaDataFile =  'WHOMetaDataDict.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getParsedHTML(url):\n",
    "    url = urllib2.urlopen(url)\n",
    "    html = url.read()\n",
    "    url.close()\n",
    "    return BeautifulSoup(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not os.path.isfile(cleanListFile):\n",
    "    with open(cleanListFile, 'w') as fp:\n",
    "        json.dump([], fp)\n",
    "\n",
    "if not os.path.isfile(metaDataFile):\n",
    "    with open(metaDataFile, 'w') as fp:\n",
    "        json.dump([], fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get HTML of list of pubs from WHO puplication list\n",
    "parsed_html = getParsedHTML(\"http://apps.who.int/iris/browse?type=dateissued&sort_by=2&order=ASC&rpp=159864&etal=0&submit_browse=Update\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find_all'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-94b4f4fdee54>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mbookUrlList\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0maList\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsed_html\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbody\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'a'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'class'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;34m'list-results'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#print aList\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find_all'"
     ]
    }
   ],
   "source": [
    "bookUrlList = []\n",
    "\n",
    "aList = parsed_html.body.find_all('a', attrs={'class':'list-results'})\n",
    "\n",
    "#print aList\n",
    "for a in aList:\n",
    "    bookUrlList.append('http://apps.who.int' + a['href'])\n",
    "\n",
    "# Check scraped list against urls for books that have already been scraped for metadata\n",
    "with open(cleanListFile) as fp:\n",
    "    cleanUrls = json.load(fp)\n",
    "\n",
    "dirtyUrlList = []\n",
    "# If book already scraped, delete\n",
    "for i, url in enumerate(bookUrlList):\n",
    "    if url not in cleanUrls:\n",
    "        dirtyUrlList.append(url)\n",
    "\n",
    "# Save resulting urls, these must be books we haven't scraped yet\n",
    "with open(dirtyListFile, 'wb') as fp:\n",
    "    json.dump(dirtyUrlList, fp)\n",
    "\n",
    "print len(dirtyUrlList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load list of books we haven't scraped yet\n",
    "with open(dirtyListFile) as fp:\n",
    "    dirtyUrlList = json.load(fp)\n",
    "    \n",
    "cleanUrlList = []\n",
    "newDirtyUrlList = []\n",
    "errorUrlList = []\n",
    "\n",
    "bookMetaData = []\n",
    "\n",
    "for i, url in enumerate(dirtyUrlList):\n",
    "    print str(float(i)/len(dirtyUrlList) * 100) + '% complete'\n",
    "    parsed_html = getParsedHTML(url)\n",
    "    metaTemp = {}\n",
    "    try:\n",
    "        if parsed_html.find(class_=\"page-title\"):\n",
    "            metaTemp[\"title\"] = parsed_html.find(class_=\"page-title\").h1.string\n",
    "            metaTemp['desc'] =  parsed_html.find(class_=\"page-title\").p.string\n",
    "        else:\n",
    "            print \"No title: \" + url\n",
    "        pubData = parsed_html.find(class_=\"pub-data\").find_all('tr')\n",
    "        for tr in pubData:\n",
    "            metaTemp[tr.th.string[:-2].lower()] = tr.td.string\n",
    "        if (parsed_html.find(id='download')):\n",
    "            metaTemp['downloadURL'] = 'http://www.ilo.org' + parsed_html.find(id='download').a['href']\n",
    "        else:\n",
    "            print \"No Download: \" + url\n",
    "        metaTemp['originURL'] = url\n",
    "    except:\n",
    "        # If there's an error, put it in error list\n",
    "        errorUrlList.append(url)\n",
    "        print 'dirty: ' + url\n",
    "        print \"Unexpected error:\", sys.exc_info()[0]\n",
    "    else:\n",
    "        # if there's no error, put the url on the clean list & save metadata\n",
    "        cleanUrlList.append(url)\n",
    "        bookMetaData.append(metaTemp)\n",
    "\n",
    "with open(dirtyListFile, 'wb') as fp:\n",
    "    json.dump(newDirtyUrlList, fp)\n",
    "\n",
    "with open(cleanListFile, 'wb') as fp:\n",
    "    json.dump(cleanUrlList, fp)\n",
    "\n",
    "# Load in previous metaData\n",
    "with open(metaDataFile) as fp:\n",
    "    prevMetaData = json.load(fp)\n",
    "# And append new data\n",
    "with open(metaDataFile, 'w') as fp:\n",
    "    json.dump(bookMetaData + prevMetaData, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(metaDataFile) as fp:\n",
    "    prevMetaData = json.load(fp)\n",
    "\n",
    "downloadable = []\n",
    "for data in prevMetaData:\n",
    "    if \"downloadURL\" in data:\n",
    "        data['UUID'] = str(uuid.uuid1())\n",
    "        downloadable.append(data)\n",
    "print len(downloadable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for data in downloadable[:3]:\n",
    "    url = data['downloadURL']\n",
    "    urllib.urlretrieve(url, data['UUID'] + '.' + url.rsplit('.')[-1])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
